<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>神经网络基础实验</title>
    <link href="/2022/08/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E5%AE%9E%E9%AA%8C/"/>
    <url>/2022/08/31/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E5%AE%9E%E9%AA%8C/</url>
    
    <content type="html"><![CDATA[<p>本文介绍了基于numpy的回归神经网络,以及基于pytorch的回归神经网络和分类神经网络</p><span id="more"></span><h1 id="目录"><a class="markdownIt-Anchor" href="#目录"></a> 目录</h1><p><a href="#%E5%9F%BA%E4%BA%8Enumpy%E7%9A%84%E5%9B%9E%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">基于numpy的回归神经网络</a><br /><a href="#%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E5%9B%9E%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">基于pytorch的回归神经网络</a><br /><a href="#%E5%9F%BA%E4%BA%8Epytorch%E7%9A%84%E5%88%86%E7%B1%BB%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">基于pytorch的分类神经网络</a></p><hr /><h2 id="基于numpy的回归神经网络"><a class="markdownIt-Anchor" href="#基于numpy的回归神经网络"></a> 基于numpy的回归神经网络</h2><p><strong>实验目的</strong><br />1.了解反向传播网络的基本原理；<br />2.了解梯度下降法进行神经网络中的权值更新；<br />3.学习使用 numpy 编写简单的三层回归网络进行回归实验。<br /><strong>实验内容</strong><br />使用 numpy 库构建简单的数据集，编写简单的三层回归神经网络，输入和输出只有一个神经元，中间隐藏层可设置 N 个神经元，采用 sigmoid 函数作为激活函数。数据集按 y=x+随机噪声进行构建，x 取值为 0, 1, …, 19。学习梯度计算，及梯度下降法进行神经网络的权值修改。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid_derivative</span>(<span class="hljs-params">s</span>):  <span class="hljs-comment">#对sigmoid进行求导</span><br>    ds = s*(<span class="hljs-number">1</span>-s)<br>    <span class="hljs-keyword">return</span> ds<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">sigmoid</span>(<span class="hljs-params">x</span>):   <br>    s=<span class="hljs-number">1</span>/(<span class="hljs-number">1</span>+np.exp(-x))<br>    <span class="hljs-keyword">return</span> s<br></code></pre></td></tr></table></figure><ul><li>激活函数sigmoid函数也叫Logistic函数，是一个在生物学上常见的S型函数,用于隐层神经元输出，取值范围为(0,1)，它可以将一个实数映射到(0,1)的区间，可以用来做二分类。在特征相差比较复杂或是相差不是特别大时效果比较好。Sigmoid作为激活函数有以下优缺点：</li><li><ul><li>优点：平滑、易于求导。</li></ul></li><li><ul><li>缺点：激活函数计算量大，反向传播求误差梯度时，求导涉及除法；反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练。<br /><img src="https://s1.imagehub.cc/images/2022/08/31/Snipaste_2022-08-31_17-34-12.png" alt="" /></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">N,D_in,H,D_out = <span class="hljs-number">20</span>,<span class="hljs-number">1</span>,<span class="hljs-number">64</span>,<span class="hljs-number">1</span><br>np.random.seed(<span class="hljs-number">0</span>)<br>x=np.arange(<span class="hljs-number">0</span>,N,<span class="hljs-number">1</span>).reshape(N,D_in)*<span class="hljs-number">1.0</span><br>y=x+np.random.randn(N,D_out)<br></code></pre></td></tr></table></figure><p>生成随机数据,x为一个[20,1]的从0-19数据,y为随机的[1,20]的数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">w1=np.random.randn(D_in,H)<br>w2=np.random.randn(H,D_out)<br></code></pre></td></tr></table></figure><p>随机生成两个参数w1,w2</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python">learning_rate=<span class="hljs-number">1e-4</span> <span class="hljs-comment">#学习率</span><br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">20000</span>): <span class="hljs-comment">#反向传播算法</span><br>    y1=x.dot(w1) <span class="hljs-comment">#x和w1进行点积 得到y1  20*64 第一层</span><br>    y_relu=sigmoid(y1) <span class="hljs-comment">#将y1输入激活函数得到f(y1) 20*64</span><br>    y2=y_relu.dot(w2) <span class="hljs-comment">#f(y)和w2进行点积得到y2  20*1 第二层</span><br>    loss=np.square(y2-y).<span class="hljs-built_in">sum</span>() <span class="hljs-comment">#损失函数</span><br>    grad_y2=<span class="hljs-number">2.0</span>*(y2-y) <span class="hljs-comment">#损失函数对y2的偏导数 20*1</span><br>    grad_w2=y_relu.T.dot(grad_y2) <span class="hljs-comment"># 损失函数对w2的偏导数 y_relu的转置(64*20)与grad_y2(20*1)的点积 (64*1)</span><br>    grad_y1=grad_y2.dot(w2.T) <br>    grad_y1=grad_y1*sigmoid_derivative(y_relu)<br>    grad_w1=x.T.dot(grad_y1) <span class="hljs-comment">#损失函数对w1的偏导数</span><br>    w1-=learning_rate*grad_w1<br>    w2-=learning_rate*grad_w2<br>    <span class="hljs-keyword">if</span>(t%<span class="hljs-number">2000</span>==<span class="hljs-number">0</span>):<br>        plt.cla()<br>        plt.scatter(x,y)<br>        plt.scatter(x,y2)<br>        plt.plot(x,y2,<span class="hljs-string">&#x27;r-&#x27;</span>,lw=<span class="hljs-number">1</span>,label=<span class="hljs-string">&quot;plot figure&quot;</span>)<br>        plt.text(<span class="hljs-number">0.5</span>,<span class="hljs-number">0</span>,<span class="hljs-string">&#x27;t=%d:Loss=%.4f&#x27;</span>%(t,loss),fontdict=&#123;<span class="hljs-string">&#x27;size&#x27;</span>:<span class="hljs-number">20</span>,<span class="hljs-string">&#x27;color&#x27;</span>:<span class="hljs-string">&#x27;red&#x27;</span>&#125;)<br>        plt.show()<br></code></pre></td></tr></table></figure><p>用反向传播算法得到<strong>w1和w2的梯度</strong>,再根据学习率对w1和w2进行适当的更新</p><ul><li><strong>反向传播算法:</strong> 简称BP算法，适合于多层神经元网络的一种学习算法，它建立在<strong>梯度下降法</strong>的基础上。BP算法的学习过程由正向传播过程和反向传播过程组成。在正向传播过程中，输入信息通过输入层经隐含层，逐层处理并传向输出层。如果在输出层得不到期望的输出值，则取输出与期望的误差的平方和作为目标函数，转入反向传播，逐层求出目标函数对各神经元权值的偏导数，构成目标函数对权值向量的梯量，作为修改权值的依据，网络的学习在权值修改过程中完成。误差达到所期望值时，网络学习结束。</li><li><strong>梯度下降法:</strong> 绝大多数的机器学习模型都会有一个损失函数。比如常见的均方误差（Mean Squared Error)损失函数,梯度下降的目的，就是为了<strong>最小化损失函数</strong>。根据损失函数对w1和w2求导得到的梯度,我们知道该往梯度方向更新参数来减小损失函数:<br /><img src="https://s1.imagehub.cc/images/2022/08/31/Snipaste_2022-08-31_18-50-01.png" alt="" /><br />但是更新不宜过多,也不宜过少,过多会直接越过最小值,过小会使学习效率过低,这里我们用到学习率(Learning Rate)这个概念。通过学习率，可以计算前进的距离。</li></ul><p>代码运行结果如下:<br /><img src="https://s1.imagehub.cc/images/2022/08/31/out1put.png" alt="" /><br /><img src="https://s1.imagehub.cc/images/2022/08/31/ou2tput.png" alt="" /><br /><img src="https://s1.imagehub.cc/images/2022/08/31/out3put.png" alt="" /><br /><img src="https://s1.imagehub.cc/images/2022/08/31/outpu4t.png" alt="" /></p><hr /><h2 id="基于pytorch的回归神经网络"><a class="markdownIt-Anchor" href="#基于pytorch的回归神经网络"></a> 基于pytorch的回归神经网络</h2><p>上面我们看到基于numpy实现的回归神经网络,但如果修改学习率,很大概率会梯度消失,难以拟合.而pytorch封装的函数对模型的训练效果会更好.</p><p><strong>实验目的：</strong></p><ol><li>了解 pytorch 下采用 adam 梯度下降法进行神经网络中的权值更新；</li><li>学习使用 pytorch 编写简单的三层神经网络进行回归实验。</li></ol><p><strong>实验内容：</strong><br />使用 numpy 库构建简单的数据集，编写简单的三层回归神经网络，输入和输出只有一个神经元，中间隐藏层可设置 N 个神经元，采用 Sigmoid 函数作为激活函数。数据集按 y=x+随机噪声进行构建，x 取值为 0, 1, …, 19。了解 pytorch 中的梯度计算，及梯度下降法进行神经网络的权值更新。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>N, D_in, H, D_out = <span class="hljs-number">50</span>, <span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">1</span><br>np.random.seed(<span class="hljs-number">0</span>)<br>x = torch.tensor(np.arange(<span class="hljs-number">0</span>,N,<span class="hljs-number">1</span>).reshape(N,D_in),dtype=torch.float32) <br>y = x +torch.tensor(np.random.randn(N,D_out), dtype=torch.float32)<br></code></pre></td></tr></table></figure><p>生成训练集。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">model = torch.nn.Sequential(<br>torch.nn.Linear(D_in, H), torch.nn.ReLU(), torch.nn.Linear(H, D_out), )<span class="hljs-comment">#构建模型 两层全连接层</span><br></code></pre></td></tr></table></figure><ul><li>torch.nn.Sequential是一个Sequential容器，模块将按照构造函数中传递的顺序添加到模块中。通俗的话说，就是根据自己的需求，把不同的函数组合成一个（小的）模块使用或者把组合的模块添加到自己的网络中。</li><li>PyTorch的nn.Linear（）用于设置神经网络中的全连接层。</li><li>此处用的激活函数是ReLU。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">loss_fn = torch.nn.MSELoss(reduction=<span class="hljs-string">&#x27;sum&#x27;</span>) <span class="hljs-comment">#损失函数</span><br></code></pre></td></tr></table></figure><ul><li>torch.nn.MSELoss()用于测量输入x和目标y中每个元素之间的均方误差。reduction为’sum’表示求和,'mean’表示求平均</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">learning_rate = <span class="hljs-number">1e-4</span> <span class="hljs-comment">#学习率</span><br>optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) <span class="hljs-comment">#优化器：优化参数 model.parameters()传入参数</span><br></code></pre></td></tr></table></figure><ul><li>model.parameters()保存的是Weights和Bais参数的值。</li><li>torch.optim是一个实现了多种优化算法的包，大多数通用的方法都已支持，提供了丰富的接口调用，未来更多精炼的优化算法也将整合进来。<br />为了使用torch.optim，需先构造一个优化器对象Optimizer，用来保存当前的状态，并能够根据计算得到的梯度来更新参数。</li><li>Adam(Adaptive Moment Estimation)本质上是带有动量项的RMSprop，它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。它的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5000</span>):<br>    y_pred = model(x)<br>    loss = loss_fn(y_pred, y)<br>    <span class="hljs-keyword">if</span> t % <span class="hljs-number">1000</span> == <span class="hljs-number">0</span>:<br>        plt.cla()<br>        plt.scatter(x.data.numpy(),y.data.numpy())<br>        plt.scatter(x.data.numpy(),y_pred.data.numpy())<br>        plt.plot(x.data.numpy(),y_pred.data.numpy(),<span class="hljs-string">&#x27;r-&#x27;</span>,lw=<span class="hljs-number">1</span>, label=<span class="hljs-string">&quot;plot figure&quot;</span>)<br>        plt.text(<span class="hljs-number">0.5</span>, <span class="hljs-number">0</span>, <span class="hljs-string">&#x27;t=%d:Loss=%.4f&#x27;</span> % (t, loss), fontdict=&#123;<span class="hljs-string">&#x27;size&#x27;</span>: <span class="hljs-number">20</span>, <span class="hljs-string">&#x27;color&#x27;</span>: <span class="hljs-string">&#x27;red&#x27;</span>&#125;)<br>        plt.show()<br>    optimizer.zero_grad() <span class="hljs-comment">#清空过往梯度</span><br>    loss.backward() <span class="hljs-comment">#反向传播，计算当前梯度</span><br>    optimizer.step() <span class="hljs-comment"># 根据梯度更新网络参数</span><br></code></pre></td></tr></table></figure><p>计算预估的y值y_pred,求出损失值,计算梯度,由此更新参数,反复训练,得到结果:<br /><img src="https://s1.imagehub.cc/images/2022/08/31/1output.png" alt="" /><br /><img src="https://s1.imagehub.cc/images/2022/08/31/2output.png" alt="" /><br /><img src="https://s1.imagehub.cc/images/2022/08/31/3output.png" alt="" /><br /><img src="https://s1.imagehub.cc/images/2022/08/31/4output.png" alt="" /></p><hr /><h2 id="基于pytorch的分类神经网络"><a class="markdownIt-Anchor" href="#基于pytorch的分类神经网络"></a> 基于pytorch的分类神经网络</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br>n_data = torch.ones(<span class="hljs-number">100</span>, <span class="hljs-number">2</span>) <span class="hljs-comment">#生成一个全为1的矩阵</span><br>x0 = torch.normal(<span class="hljs-number">2</span>*n_data, <span class="hljs-number">1</span>) <span class="hljs-comment"># class0 x data (tensor), shape=(100, 2)</span><br>y0 = torch.zeros(<span class="hljs-number">100</span>) <span class="hljs-comment"># class0 y data (tensor), shape=(100, 1)</span><br>x1 = torch.normal(-<span class="hljs-number">2</span>*n_data, <span class="hljs-number">1</span>) <span class="hljs-comment"># class1 x data (tensor), shape=(100, 2)</span><br>y1 = torch.ones(<span class="hljs-number">100</span>) <span class="hljs-comment"># class1 y data (tensor), shape=(100, 1)</span><br>x = torch.cat((x0, x1), <span class="hljs-number">0</span>).<span class="hljs-built_in">type</span>(torch.FloatTensor) <span class="hljs-comment"># shape (200, 2) FloatTensor = 32-bit floating</span><br>y = torch.cat((y0, y1), ).<span class="hljs-built_in">type</span>(torch.LongTensor) <span class="hljs-comment"># shape (200,) LongTensor = 64-bit integer</span><br></code></pre></td></tr></table></figure><p>生成训练样本</p><ul><li>torch.ones()生成一个全为1的矩阵</li><li>torch.normal(means, std, out=None)</li><li><ul><li>means (Tensor)均值 std (Tensor)标准差 out (Tensor)可选的输出张量</li></ul></li><li>torch.cat()在给定维度上对输入的张量序列进行连接操作</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Net</span>(torch.nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, n_feature, n_hidden, n_output</span>):<br>        <span class="hljs-built_in">super</span>(Net, self).__init__()<br>        self.hidden = torch.nn.Linear(n_feature, n_hidden) <span class="hljs-comment"># hidden layer</span><br>        self.out = torch.nn.Linear(n_hidden, n_output) <span class="hljs-comment"># output layer</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = torch.sigmoid(self.hidden(x)) <span class="hljs-comment"># activation function for hidden layer</span><br>        x = self.out(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><ul><li>在定义自已的网络的时候，需要继承nn.Module类，并重新实现构造函数__init__构造函数和forward这两个方法。</li><li>super().<strong>init</strong>()，就是继承父类的init方法，同样可以使用super()去继承其他方法。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python">net = Net(n_feature=<span class="hljs-number">2</span>, n_hidden=<span class="hljs-number">10</span>, n_output=<span class="hljs-number">2</span>) <span class="hljs-comment"># define the network</span><br>optimizer = torch.optim.SGD(net.parameters(), lr=<span class="hljs-number">0.02</span>) <br>loss_func = torch.nn.CrossEntropyLoss()<br>plt.ion()<br><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">100</span>):<br>    out = net(x) <span class="hljs-comment"># 预测的值</span><br>    loss = loss_func(out, y) <span class="hljs-comment"># 计算损失</span><br>    optimizer.zero_grad() <span class="hljs-comment"># 清空梯度</span><br>    loss.backward() <span class="hljs-comment"># 反向传播</span><br>    optimizer.step() <span class="hljs-comment"># 更新梯度</span><br>    <span class="hljs-keyword">if</span> t % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>:<br>        plt.cla()<br>        prediction = torch.<span class="hljs-built_in">max</span>(out, <span class="hljs-number">1</span>)[<span class="hljs-number">1</span>]<br>        pred_y = prediction.data.numpy()<br>        target_y = y.data.numpy()<br>        plt.scatter(x.data.numpy()[:, <span class="hljs-number">0</span>], x.data.numpy()[:, <span class="hljs-number">1</span>], c=pred_y, s=<span class="hljs-number">100</span>, lw=<span class="hljs-number">0</span>, cmap=<span class="hljs-string">&#x27;RdYlGn&#x27;</span>)<br>        accuracy = <span class="hljs-built_in">float</span>((pred_y == target_y).astype(<span class="hljs-built_in">int</span>).<span class="hljs-built_in">sum</span>()) / <span class="hljs-built_in">float</span>(target_y.size)<br>        plt.text(<span class="hljs-number">1.5</span>, -<span class="hljs-number">4</span>, <span class="hljs-string">&#x27;Accuracy=%.2f&#x27;</span> % accuracy, fontdict=&#123;<span class="hljs-string">&#x27;size&#x27;</span>: <span class="hljs-number">20</span>, <span class="hljs-string">&#x27;color&#x27;</span>: <span class="hljs-string">&#x27;red&#x27;</span>&#125;)<br>        plt.pause(<span class="hljs-number">0.1</span>)<br>plt.ioff()<br>plt.show()<br></code></pre></td></tr></table></figure><p>运行结果如下:<br /><img src="https://s1.imagehub.cc/images/2022/08/31/outputq.png" alt="" /><br /><img src="https://s1.imagehub.cc/images/2022/08/31/outpzut.png" alt="" /><br /><img src="https://s1.imagehub.cc/images/2022/08/31/outxput.png" alt="" /></p>]]></content>
    
    
    
    <tags>
      
      <tag>人工智能</tag>
      
      <tag>神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>机器学习基础实验</title>
    <link href="/2022/08/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E5%AE%9E%E9%AA%8C/"/>
    <url>/2022/08/30/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E5%AE%9E%E9%AA%8C/</url>
    
    <content type="html"><![CDATA[<p>本文介绍了监督学习和无监督学习的经典算法：线性回归以及k-means聚类算法。</p><span id="more"></span><h1 id="目录"><a class="markdownIt-Anchor" href="#目录"></a> 目录</h1><p><a href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0">线性回归（监督学习）</a><br /><a href="#k-means%E8%81%9A%E7%B1%BB%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0">k-means聚类（无监督学习）</a></p><hr /><h1 id="线性回归监督学习"><a class="markdownIt-Anchor" href="#线性回归监督学习"></a> 线性回归（监督学习）</h1><p><strong>实验目的：</strong></p><p>1.了解线性回归的基本原理<br />2.掌握通过梯度下降方法实现最优解的求解</p><p><strong>实验环境：</strong> python，sklearn，numpy</p><p><strong>实验步骤：</strong></p><p>1、生成数据<br />2、定义画图函数、假设函数、损失函数、梯度计算函数、参数更新函数<br />3、定义线性回归函数，并测试</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># %matplotlib inline</span><br><span class="hljs-keyword">from</span> sklearn.datasets <span class="hljs-keyword">import</span> make_regression <span class="hljs-comment">#导入 make_regression()函数，用来生成回归</span><br>模型数据<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt <span class="hljs-comment">#导入 matplotlib.pyplot，并且重命名为 plt</span><br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np <span class="hljs-comment">#导入 numpy 库，并且重命名为 np</span><br></code></pre></td></tr></table></figure><ul><li>函数 make_regression()：用来生成回归模型数据</li><li>参数说明：<br />n_samples：样本数<br />n_features：特征数<br />noise：噪音<br />bias：偏差</li><li>X : array of shape [n_samples, n_features]<br />y : array of shape [n_samples] or [n_samples, n_targets]</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">X, y= make_regression(n_samples=<span class="hljs-number">100</span>, n_features=<span class="hljs-number">1</span>, noise=<span class="hljs-number">0.4</span>, bias=<span class="hljs-number">50</span>)<br></code></pre></td></tr></table></figure><p>定义一个名为 plotLine()的函数，用来画出生成数据集的散点图和拟合线性模型(y=k*x+b)</p><ul><li><p>参数说明：<br />theta0:即 y=k<em>x+b 中的参数 b<br />theta1:即 y=k</em>x+b 中的参数 k<br />X:数据集的横坐标（列表类型）<br />y:数据集的纵坐标（列表类型）</p></li><li><p>np.linspace(start, stop, num)函数：用来返回 num 个等间距的样本，在区间[start, stop]中。</p></li><li><p>plt.plot(x,y,color,label)：可视化函数<br />参数说明：color:用来设置线条的颜色，color='r’表示红色(b 表示蓝色)；label 用于指定标签</p></li><li><p>plt.scatter(x,y)：用来画散点图。</p></li><li><p>plt.axis(）函数用来指定坐标轴的范围。<br />参数需要以列表的形式给出。</p></li><li><p>plt.show()：将图像显示出。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">plotLine</span>(<span class="hljs-params">theta0, theta1, X, y</span>):<br>    max_x = np.<span class="hljs-built_in">max</span>(X) + <span class="hljs-number">100</span> <span class="hljs-comment">#np.max(X)用来取出 X 中的最大值</span><br>    min_x = np.<span class="hljs-built_in">min</span>(X) - <span class="hljs-number">100</span> <span class="hljs-comment">#np.min(X)用来取出 X 中的最小值</span><br>    xplot = np.linspace(min_x, max_x, <span class="hljs-number">1000</span>) <span class="hljs-comment">#在区间[min_x,max_x]中返回 1000 个等间隔的样本</span><br>    yplot = theta0 + theta1 * xplot <span class="hljs-comment">#将 x 带入线性方程 y=k*x+b 中求得 y</span><br>    <span class="hljs-built_in">print</span>(theta0) <span class="hljs-comment">#打印参数 theta0</span><br>    <span class="hljs-built_in">print</span>(theta1) <span class="hljs-comment">#打印参数 theta1</span><br>    plt.plot(xplot, yplot, color=<span class="hljs-string">&#x27;g&#x27;</span>, label=<span class="hljs-string">&#x27;Regression Line&#x27;</span>) <span class="hljs-comment">#画出线性模型，参数依次表示：横坐标，纵坐标，颜色，标签</span><br>    plt.scatter(X,y) <span class="hljs-comment">#画散点图，参数依次表示横坐标、纵坐标</span><br>    plt.axis([-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, <span class="hljs-number">0</span>, <span class="hljs-number">200</span>]) <span class="hljs-comment">#设置横坐标范围为【-10，10】，纵轴范围为【0，200】</span><br>    plt.show() <span class="hljs-comment">#显示可视化图像</span><br></code></pre></td></tr></table></figure><p>定义一个函数返回y的估计值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">hypothesis</span>(<span class="hljs-params">theta0, theta1, x</span>):<br>    <span class="hljs-keyword">return</span> theta0 + (theta1 * x)<br></code></pre></td></tr></table></figure><p>定义损失函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">cost</span>(<span class="hljs-params">theta0, theta1, X, y</span>):  <span class="hljs-comment"># 计算损失</span><br>    costValue = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> (xi, yi) <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(X, y):  <span class="hljs-comment"># 使用 zip()函数，包为元组的列表</span><br>        costValue += <span class="hljs-number">0.5</span> * ((hypothesis(theta0, theta1, xi) - yi) ** <span class="hljs-number">2</span>)<br>        <span class="hljs-keyword">return</span> costValue<br></code></pre></td></tr></table></figure><p>定义名为 derivatives()的函数，用来计算参数的梯度：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">derivatives</span>(<span class="hljs-params">theta0, theta1, X, y</span>): <span class="hljs-comment">#derivative:导数</span><br>    dtheta0 = <span class="hljs-number">0</span> <span class="hljs-comment">#dtheta0：参数 theta0 的梯度，初始化为 0</span><br>    dtheta1 = <span class="hljs-number">0</span> <span class="hljs-comment">#dtheta0：参数 theta0 的梯度，初始化为 0</span><br>    <span class="hljs-keyword">for</span> (xi, yi) <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(X, y): <span class="hljs-comment">#使用 zip()函数依次取出(xi,yi)</span><br>        dtheta0 += hypothesis(theta0, theta1, xi) - yi<br>        dtheta1 += (hypothesis(theta0, theta1, xi) - yi)*xi <span class="hljs-comment">#计算公式为：损失函数对参数</span><br>    dtheta0 /= <span class="hljs-built_in">len</span>(X) <span class="hljs-comment">#求平均梯度，len(X)函数用来计算 X 中的样本数</span><br>    dtheta1 /= <span class="hljs-built_in">len</span>(X) <span class="hljs-comment">#求平均梯度</span><br>    <span class="hljs-keyword">return</span> dtheta0,dtheta1<br></code></pre></td></tr></table></figure><p>定义一个名为 updateParameters()的函数，用来对参数进行更新:</p><ul><li>参数说明：<br />theta0 和 theta1 为待更新参数。<br />X、 y 分别表示横轴和纵轴的数值。<br />alpha：学习率。</li><li>参数的更新：<br />对于参数 w，其更新方式为：w=w-学习率*梯度值。其中学习率是一个超参数</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">updateParameters</span>(<span class="hljs-params">theta0, theta1, X, y, alpha</span>):  <span class="hljs-comment"># 参数的更新，alpha 表示学习率</span><br>    dtheta0, dtheta1 = derivatives(theta0, theta1, X, y)  <span class="hljs-comment"># dtheta0, dtheta1 分 别 表 示 参 数</span><br>    theta0 = theta0 - (alpha * dtheta0)  <span class="hljs-comment"># 依据参数更新方式更新参数 theta0</span><br>    theta1 = theta1 - (alpha * dtheta1)  <span class="hljs-comment"># 依据参数更新方式更新参数 theta1</span><br>    <span class="hljs-keyword">return</span> theta0, theta1  <span class="hljs-comment"># 返回更新好的参数</span><br></code></pre></td></tr></table></figure><p>定义并调用一个名为 LinearRegression()的线性回归函数;</p><h2 id="-code7-实验结果如下g1g2g3g4g5g6g7"><a class="markdownIt-Anchor" href="#-code7-实验结果如下g1g2g3g4g5g6g7"></a> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">LinearRegression</span>(<span class="hljs-params">X, y</span>):<br>    theta0 = np.random.rand()  <span class="hljs-comment"># 给 theta0 赋一个随机初始值。</span><br>    theta1 = np.random.rand()  <span class="hljs-comment"># 给 theta1 赋一个随机初始值。</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-number">1000</span>):  <span class="hljs-comment"># 进行 1000 次参数的更新，每隔 100 次跟新打印一次图片</span><br>        <span class="hljs-keyword">if</span> i % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:  <span class="hljs-comment"># 只有当 i 整除 100 时才进行一次图片打印</span><br>            plotLine(theta0, theta1, X, y)<br>            <span class="hljs-built_in">print</span>(cost(theta0, theta1, X, y))<br>        theta0, theta1 = updateParameters(theta0, theta1, X, y, <span class="hljs-number">0.005</span>)<br>LinearRegression(X, y) <span class="hljs-comment">#调用线性回归函数。</span><br></code></pre></td></tr></table></figure><br />实验结果如下：<br /><img src="https://s1.imagehub.cc/images/2022/08/30/Snipaste_2022-08-30_18-27-30.png" alt="g1" /><br /><img src="https://s1.imagehub.cc/images/2022/08/30/Snipaste_2022-08-30_18-27-52.png" alt="g2" /><br /><img src="https://s1.imagehub.cc/images/2022/08/30/Snipaste_2022-08-30_18-28-21.png" alt="g3" /><br /><img src="https://s1.imagehub.cc/images/2022/08/30/Snipaste_2022-08-30_18-28-35.png" alt="g4" /><br /><img src="https://s1.imagehub.cc/images/2022/08/30/Snipaste_2022-08-30_18-28-42.png" alt="g5" /><br /><img src="https://s1.imagehub.cc/images/2022/08/30/Snipaste_2022-08-30_18-28-54.png" alt="g6" /><br /><img src="https://s1.imagehub.cc/images/2022/08/30/Snipaste_2022-08-30_18-29-06.png" alt="g7" /></h2><p><strong>原理</strong><br />给定一组样本观测值x<sub>i</sub>,y<sub>i</sub>，(i=1,2,…n)，要求回归函数尽可能拟合这组值。普通最小二乘法给出的判断标准是：残差平方和的值达到最小。<br />残差平方和的公式为：<br /><img src="https://s1.imagehub.cc/images/2022/08/30/Snipaste_2022-08-30_18-43-27.png" alt="g8" /><br />有两个未知参数的二次方程，画出来是一个三维空间中的图像，类似下面：<br /><img src="https://s1.imagehub.cc/images/2022/08/30/Snipaste_2022-08-30_18-43-38.png" alt="g9" /><br />导数为0时，Q取最小值，因此我们分别对\beta<sub>1</sub>和\beta<sub>2</sub>求偏导并令其为0。</p><h1 id="k-means聚类无监督学习"><a class="markdownIt-Anchor" href="#k-means聚类无监督学习"></a> k-means聚类（无监督学习）</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> numpy <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">import</span> time<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br></code></pre></td></tr></table></figure><p>定义一个名为 euclDistance()的函数，用来计算两个矩阵之间的欧式距离。其中参数vector1, vector2 分别表示两个矩阵。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">euclDistance</span>(<span class="hljs-params">vector1, vector2</span>):<br>    <span class="hljs-keyword">return</span> sqrt(<span class="hljs-built_in">sum</span>(power(vector2 - vector1, <span class="hljs-number">2</span>))) <span class="hljs-comment">#求这两个矩阵的距离，vector1、2 均为矩阵</span><br></code></pre></td></tr></table></figure><p>定义一个名为 initCentroids()的函数，用来在样本集中随机选取 k 个样本点作为初始<br />质心，其中参数 dataSet 为已给数据集，k 表示创建中心点的个数。返回值为所创建的 k 个中心点</p><ul><li>np.zeros([k, n])：用来创建一个 k 行 n 列的全 0 数组。</li><li>np.random.uniform(a,b):返回区间[a,b)中的任意值。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#在样本集中随机选取 k 个样本点作为初始质心。</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">initCentroids</span>(<span class="hljs-params">dataSet, k</span>):<br>    numSamples, dim = dataSet.shape <span class="hljs-comment">#矩阵的行数、列数 。</span><br>    centroids = zeros((k, dim)) <span class="hljs-comment"># 创建一个 k 行 dim 列的全 0 数组。</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(k):<br>        index = <span class="hljs-built_in">int</span>(random.uniform(<span class="hljs-number">0</span>, numSamples)) <span class="hljs-comment">#随机产生一个浮点数，然后将其转化为 int 型。</span><br>        centroids[i, :] = dataSet[index, :] <span class="hljs-comment"># 将 dataSet 中 第 index+1 行 赋 值 给centroids 的第 i+1 行。</span><br>    <span class="hljs-keyword">return</span> centroids<br></code></pre></td></tr></table></figure><p>定义一个名为 kmeans()的聚类算法，用于将 dataSet 矩阵中的样本分成 k 个类</p><ul><li>np.mat(a):用于将数组 a 转换为矩阵。</li><li>np.zeros([k, n])：用来创建一个 k 行 n 列的全 0 数组。</li><li>matrix.A:将矩阵类型转换为 array 类型。</li><li>np.nonzero(array):用于得到数组 array 中非零元素的位置（数组索引）,参数 array 为<br />一个数组。</li><li>np.mean()：求均值。</li><li>plt.plot(x,y,color,marksize):当使用此函数画一个数据点时，参数 x 表示横坐标,参数 y 表示纵坐标，参数 color 用来指定点的颜色，参数 marksize 用来指示点的大小。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">kmeans</span>(<span class="hljs-params">dataSet, k</span>):<br>    numSamples = dataSet.shape[<span class="hljs-number">0</span>] <span class="hljs-comment">#读取矩阵 dataSet 的第一维度的长度,即获得有多少个样本数据</span><br>    clusterAssment = mat(zeros((numSamples, <span class="hljs-number">2</span>))) <span class="hljs-comment">#得到一个 N*2 的零矩阵,建立簇分配结果矩阵，第一列存类别，第二列存误差。</span><br>    clusterChanged = <span class="hljs-literal">True</span> <span class="hljs-comment">#用来判断样本聚类结果是否变化的变量。</span><br>    <span class="hljs-comment">## step 1: init centroids</span><br>    centroids = initCentroids(dataSet, k) <span class="hljs-comment">#在样本集中随机选取 k 个样本点作为初始质心</span><br>    <span class="hljs-keyword">while</span> clusterChanged:<br>        clusterChanged = <span class="hljs-literal">False</span><br>        <span class="hljs-comment">## for each sample</span><br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(numSamples): <span class="hljs-comment">#range</span><br>            minDist = <span class="hljs-number">100000.0</span> <span class="hljs-comment">#创建的一个临时变量，用来储存某个样本到所有聚类中心的最小距离。</span><br>            minIndex = <span class="hljs-number">0</span> <span class="hljs-comment">#创建的一个临时变量，用来储存和某个样本距离最近的聚类中心的类别作为该样本的类别。</span><br>            <span class="hljs-comment">## for each centroid</span><br>            <span class="hljs-comment">## step 2: find the centroid who is closest</span><br>            <span class="hljs-comment">#计算每个样本点与质点之间的距离，将其归内到距离最小的那一簇</span><br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(k):<br>                distance = euclDistance(centroids[j, :], dataSet[i, :]) <span class="hljs-comment">#计算每个样本到每个聚类中心之间的距离。</span><br>                <span class="hljs-keyword">if</span> distance &lt; minDist:<br>                    minDist = distance<br>                    minIndex = j<br>            <span class="hljs-comment">## step 3: update its cluster</span><br>            <span class="hljs-comment">#k 个簇里面与第 i 个样本距离最小的的标号和距离保存在 clusterAssment中</span><br>            <span class="hljs-comment">#若所有的样本所属类别不在变化，则退出 while 循环</span><br>            <span class="hljs-keyword">if</span> clusterAssment[i, <span class="hljs-number">0</span>] != minIndex:<br>                clusterChanged = <span class="hljs-literal">True</span><br>                clusterAssment[i, :] = minIndex, minDist**<span class="hljs-number">2</span> <span class="hljs-comment">#两个**表示的是 minDist的平方</span><br>        <span class="hljs-comment">## step 4: update centroids</span><br>        <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(k):<br>            <span class="hljs-comment">#clusterAssment[:,0].A==j 是找出矩阵 clusterAssment 中第一列元素中等于j 的行的下标，返回的是一个以 array 的列表，第一个 array 为等于 j 的下标</span><br>            pointsInCluster = dataSet[nonzero(clusterAssment[:, <span class="hljs-number">0</span>].A == j)[<span class="hljs-number">0</span>]] <span class="hljs-comment">#将 dataSet矩阵中相对应的样本提取出来</span><br>            centroids[j, :] = mean(pointsInCluster, axis = <span class="hljs-number">0</span>) <span class="hljs-comment">#计算标注为 j 的所有样本的平均值</span><br>    <span class="hljs-built_in">print</span> (<span class="hljs-string">&#x27;Congratulations, cluster complete!&#x27;</span>)<br>    <span class="hljs-keyword">return</span> centroids, clusterAssment<br><span class="hljs-comment"># show your cluster only available with 2-D data</span><br><span class="hljs-comment">#centroids 为 k 个类别，其中保存着每个类别的质心</span><br><span class="hljs-comment">#clusterAssment 为样本的标记，第一列为此样本的类别号，第二列为到此类别质心的距离</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">showCluster</span>(<span class="hljs-params">dataSet, k, centroids, clusterAssment</span>):<br>    numSamples, dim = dataSet.shape<br>    <span class="hljs-keyword">if</span> dim != <span class="hljs-number">2</span>:<br>        <span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;Sorry! I can not draw because the dimension of your data is not 2!&quot;</span>)<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">1</span><br>    mark = [<span class="hljs-string">&#x27;or&#x27;</span>, <span class="hljs-string">&#x27;ob&#x27;</span>, <span class="hljs-string">&#x27;og&#x27;</span>, <span class="hljs-string">&#x27;ok&#x27;</span>, <span class="hljs-string">&#x27;^r&#x27;</span>, <span class="hljs-string">&#x27;+r&#x27;</span>, <span class="hljs-string">&#x27;sr&#x27;</span>, <span class="hljs-string">&#x27;dr&#x27;</span>, <span class="hljs-string">&#x27;&lt;r&#x27;</span>, <span class="hljs-string">&#x27;pr&#x27;</span>] <span class="hljs-comment">#样本颜色</span><br>    <span class="hljs-keyword">if</span> k &gt; <span class="hljs-built_in">len</span>(mark):<br>        <span class="hljs-built_in">print</span> (<span class="hljs-string">&quot;Sorry! Your k is too large! please contact wojiushimogui&quot;</span>)<br>        <span class="hljs-keyword">return</span> <span class="hljs-number">1</span><br>    <span class="hljs-comment"># draw all samples</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(numSamples):<br>        markIndex = <span class="hljs-built_in">int</span>(clusterAssment[i, <span class="hljs-number">0</span>]) <span class="hljs-comment">#为样本指定颜色</span><br>        plt.plot(dataSet[i, <span class="hljs-number">0</span>], dataSet[i, <span class="hljs-number">1</span>], mark[markIndex]) <span class="hljs-comment">#画出样本</span><br>    mark = [<span class="hljs-string">&#x27;Dr&#x27;</span>, <span class="hljs-string">&#x27;Db&#x27;</span>, <span class="hljs-string">&#x27;Dg&#x27;</span>, <span class="hljs-string">&#x27;Dk&#x27;</span>, <span class="hljs-string">&#x27;^b&#x27;</span>, <span class="hljs-string">&#x27;+b&#x27;</span>, <span class="hljs-string">&#x27;sb&#x27;</span>, <span class="hljs-string">&#x27;db&#x27;</span>, <span class="hljs-string">&#x27;&lt;b&#x27;</span>, <span class="hljs-string">&#x27;pb&#x27;</span>] <span class="hljs-comment">#中心的颜色</span><br>    <span class="hljs-comment"># draw the centroids</span><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(k):<br>        plt.plot(centroids[i, <span class="hljs-number">0</span>], centroids[i, <span class="hljs-number">1</span>], mark[i], markersize = <span class="hljs-number">12</span>) <span class="hljs-comment">#画出中心点</span><br>    plt.show() <span class="hljs-comment">#显示图片</span><br></code></pre></td></tr></table></figure><ul><li>&quot;.txt&quot;文件的读取：<br />f=open(file_path) #其中 f 叫做文件句柄，file_path 为文件所在的路径。<br />f.readlines()函数用来读取文件中的全部内容，返回值为一个列表，列表中的每个元素为每行对应的内容。<br />f.close()用来关闭所打开的文件。</li><li>.strip()方法用于移除字符串头尾指定的字符（默认为空格或换行符）。</li><li>.split(str)方法通过指定分隔符对字符串进行切片，其中参数 str 为分隔符，返回值为<br />一个列表。</li><li>.append(obj)方法用于在列表末尾添加 obj。</li></ul><pre class="highlight"><code class="python">  <span class="hljs-comment">## step 1: 载入待聚类数据</span><span class="hljs-keyword">print</span> (<span class="hljs-string">"step 1: load data..."</span> )dataSet = [] <span class="hljs-comment">#列表，用来表示，列表中的每个元素也是一个二维的列表；这个二维列表就是一个样本，样本中包含有我们的属性值和类别号。</span><span class="hljs-comment">#与我们所熟悉的矩阵类似，最终我们将获得 N*2 的矩阵，每行元素构成了我们的训练样本的属性值和类别号</span>fileIn = open(<span class="hljs-string">"testdata.txt"</span>) <span class="hljs-comment">#"D:/testdata.txt"为数据文件所在位置的绝对路径。</span><span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> fileIn.readlines(): <span class="hljs-comment">#依次遍历每一行</span>    temp=[] <span class="hljs-comment">#定义一个缓存列表</span>    lineArr = line.strip().split(<span class="hljs-string">'\t'</span>) <span class="hljs-comment">#line.strip()把末尾的'\n'去掉，.split('\t')表示以'\t'为分隔符将字符串切片。</span>    temp.append(float(lineArr[<span class="hljs-number">0</span>])) <span class="hljs-comment">#float(a)表示将 a 转化为 float 类型。</span>    temp.append(float(lineArr[<span class="hljs-number">1</span>]))    dataSet.append(temp) <span class="hljs-comment">#向 dataSet 列表中添加元素。</span>fileIn.close() <span class="hljs-comment">#关闭刚刚打开的 testdata.txt 文件。</span><span class="hljs-comment">## step 2: 聚类中... </span><span class="hljs-keyword">print</span> (<span class="hljs-string">"step 2: clustering..."</span> )dataSet = mat(dataSet) <span class="hljs-comment">#mat()函数是 Numpy 中的库函数，将数组转化为矩阵</span>k = <span class="hljs-number">4</span>centroids, clusterAssment = kmeans(dataSet, k) <span class="hljs-comment">#调用 KMeans 文件中定义的 kmeans 方法</span><span class="hljs-comment">## step 3: 画图展示结果</span><span class="hljs-keyword">print</span> (<span class="hljs-string">"step 3: show the result..."</span> )showCluster(dataSet, k, centroids, clusterAssment)</code></pre><p>实验结果如下<br /><img src="https://s1.imagehub.cc/images/2022/08/30/output.png" alt="g10" /><br /><strong>原理</strong><br />聚类属于非监督学习，K均值聚类是最基础常用的聚类算法。它的基本思想是，通过迭代寻找K个簇（Cluster）的一种划分方案，使得聚类结果对应的损失函数最小。<br />KMeans最核心的部分就是先固定中心点，调整每个样本所属的类别来减少损失函数；再固定每个样本的类别，调整中心点继续减小损失函数。两个过程交替循环，损失函数单调递减直到最（极）小值，中心点和样本划分的类别同时收敛。</p>]]></content>
    
    
    
    <tags>
      
      <tag>人工智能</tag>
      
      <tag>机器学习</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>第一篇Blog</title>
    <link href="/2022/08/29/%E7%AC%AC%E4%B8%80%E7%AF%87Blog/"/>
    <url>/2022/08/29/%E7%AC%AC%E4%B8%80%E7%AF%87Blog/</url>
    
    <content type="html"><![CDATA[<p>简要记录github+hexo搭建个人博客的步骤</p> <span id="more"></span><h1 id="目录"><a class="markdownIt-Anchor" href="#目录"></a> 目录</h1><ul><li><a href="#%E5%AE%89%E8%A3%85hexo">安装Hexo</a></li><li><a href="#%E4%B8%8B%E8%BD%BDgit">下载Git</a></li><li><a href="#git%E7%BB%91%E5%AE%9Agithub">Git绑定Github</a><ul><li><a href="#%E7%BB%91%E5%AE%9Agithub">绑定Github</a></li><li><a href="#%E6%8F%90%E4%BA%A4%E6%96%87%E4%BB%B6%E6%9C%AC%E5%9C%B0%E6%B2%A1%E6%9C%89git%E4%BB%93%E5%BA%93">提交文件(本地没有git仓库)</a></li></ul></li><li><a href="#%E5%AE%89%E8%A3%85node.js">安装node.js</a></li><li><a href="#%E5%AE%89%E8%A3%85Hexo">安装Hexo</a></li></ul><hr /><h2 id="下载git"><a class="markdownIt-Anchor" href="#下载git"></a> 下载Git</h2><p><a href="https://git-scm.com/">Git官网</a></p><h2 id="git绑定github"><a class="markdownIt-Anchor" href="#git绑定github"></a> Git绑定Github</h2><h3 id="绑定github"><a class="markdownIt-Anchor" href="#绑定github"></a> 绑定Github</h3><p>我们要用git上传文件到GitHub首先得利用SSH<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>登录远程主机，而登录方式有两种：一种是口令登录；另一种是公钥登录。口令登录每次都要输入密码十分麻烦，而公钥登录就省去了输入密码的步骤，所以我们选择公钥授权。首先我们得在 GitHub 上添加 SSH key 配置，要想生成SSH key，就要先安装 SSH，不过我们安装了 Git Bash，其应该自带了 SSH。</p><p>然后，输入 ssh-keygen -t rsa 命令（注意空格），表示我们指定 RSA 算法生成密钥，然后敲四次回车键，之后就就会生成两个文件，分别为秘钥 id_rsa 和公钥 id_rsa.pub.。<br />接下来我们要做的事情就是把公钥 id_rsa.pub 的内容添加到 GitHub。复制公钥 id_rsa.pub 文件里的内容:<br />接下来进入我们的 GitHub 主页，先点击右上角，再点击 settings ,点击 SSH and GPG keys，再点击 New SSH key,将复制的公钥 id_rsa.pub 的内容粘贴到 key 内，再点击 Add SSH key。</p><blockquote><p>git中的复制粘贴不是 Ctrl+C 和 Ctrl+V，而是 Ctrl+insert 和Shift+insert。</p></blockquote><h3 id="提交文件本地没有git仓库"><a class="markdownIt-Anchor" href="#提交文件本地没有git仓库"></a> 提交文件(本地没有git仓库)</h3><p>进入GitHub个人主页，点击进入新建的 text 项目，点击 Clone or download，再复制地址，然后进入我们准备存储 Git 仓库的目录（在git仓库这个文件下右键打开Git Bash），接下来，输入 git clone <a href="https://github.com/yourname/text.git">https://github.com/yourname/text.git</a> ，将远程仓库 clone 到本地。<br />现在我们在git仓库创建一个 text.txt 测试文件，从此目录进入 Git Bash，输入 git status 命令查看仓库状态。text 已经是一个 Git 仓库了，而我们刚刚创建的文件 text.txt 没有被追踪，也就是没有提交到本地仓库。现在我们使用 git add 命令将文件添加到了「临时缓冲区」，再用 git commit -m “提交信息” 将其提交到本地仓库。输入 git log 命令查看仓库提交日志。现在输入 git push origin main 命令，将本地仓库提交到远程仓库，这时Github就有了text文件。</p><h3 id="安装nodejs"><a class="markdownIt-Anchor" href="#安装nodejs"></a> 安装node.js</h3><p><a href="https://nodejs.org/en/">node.js官网</a><br />安装完成可以用打开cmd检验一下是否安装成功，用 node -v 和 npm -v 命令检查版本<br /><strong>设置npm在安装全局模块时的路径和环境变量：</strong><br />因为如果不设置的话，安装模块的时候就会把模块装到C盘，占用C盘的空间，并且有可能安装好hexo后却无法使用，所以我们需要设置一下：<br />在 nodejs 文件夹中新建两个空文件夹 node_cache、node_global，打开cmd，输入如下两个命令：</p><blockquote><p>npm config set prefix “D:\nodejs\node_global”<br />n&gt;pm config set cache “D:\nodejs\node_cache”</p></blockquote><p>然后<strong>设置环境变量</strong>：在系统变量中新建一个变量名为“NODE_PATH”，值为:</p><blockquote><p>D:\nodejs\node_modules</p></blockquote><p>然后编辑用户变量里的Path，将相应npm的路径改为：</p><blockquote><p>D:\nodejs\node_global</p></blockquote><h3 id="安装hexo"><a class="markdownIt-Anchor" href="#安装hexo"></a> 安装Hexo</h3><p>Hexo就是我们的个人博客网站的框架，在安装之前，我们要先在GitHub上创立一个仓库，名称为&quot;<a href="http://yourname.github.io">yourname.github.io</a>&quot;(<strong>此处yourname必须和你github用户名相同</strong>)，访问权限选择Public。<br />接下来就是安装Hexo，首先在D盘建立一个文件夹 Blog，点开 Blog 文件夹，鼠标右键打开 Git Bush Here，输入npm命令安装Hexo：</p><blockquote><p>npm install -g hexo-cli</p></blockquote><p>安装完成后，输入 hexo init 命令初始化博客(可能会失败，多试几次)，然后输入 hexo g 静态部署，这时网页已经部署完成，输入 hexo s 命令可以查看。<br />现在回到我们的 Blog 文件夹，用笔记本打开 _config.yml 文件，下滑到文件底部，填上如下内容：</p><blockquote><p>deploy:<br />type: git<br />repository: <a href="https://github.com/yourname/yourname.github.io.git">https://github.com/yourname/yourname.github.io.git</a>  #你的仓库地址<br />branch: main</p></blockquote><p>然后回到 Blog 文件夹中，打开 Git Bash，安装Git部署插件，输入命令:</p><blockquote><p>npm install hexo-deployer-git --save</p></blockquote><p>然后分别输入以下三条命令：</p><blockquote><p>hexo clean   #清除缓存文件 db.json 和已生成的静态文件 public<br />hexo g       #生成网站静态文件到默认设置的 public 文件夹(hexo generate 的缩写)<br />hexo d       #自动生成网站静态文件，并部署到设定的仓库(hexo deploy 的缩写)</p></blockquote><p>(这三条命令在更新博客时也会用到)<br />完成以后，打开浏览器，输入 <a href="https://yourname.github.io">https://yourname.github.io</a> 就可以打开你的网页了</p><hr class="footnotes-sep" /><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>SSH（安全外壳协议，Secure Shell 的缩写）是建立在应用层基础上的安全协议。SSH 是目前较可靠，专为远程登录会话和其他网络服务提供安全性的协议，利用 SSH 协议可以有效防止远程管理过程中的信息泄露问题。简单来说，SSH就是保障你的账户安全，将你的数据加密压缩，不仅防止其他人截获你的数据，还能加快传输速度。 <a href="#fnref1" class="footnote-backref">↩︎</a></p></li></ol></section>]]></content>
    
    
    
    <tags>
      
      <tag>搭建个人网页</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
